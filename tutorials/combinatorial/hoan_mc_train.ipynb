{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Higher-Order Attention Network for Complex Classification.\n",
    "\n",
    "In this notebook we will train a HOAN for mesh classification (as defined in [HZPMG22]). We will use a benchmark dataset, shrec16, a collection of 3D meshes, to train the model to perform classification at the level of the combinatorial complex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from toponetx import CombinatorialComplex\n",
    "import toponetx.datasets as datasets\n",
    "from topomodelx.nn.combinatorial.hoan_mc_layer import HOANMCLayer\n",
    "from topomodelx.base.aggregation import Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import data ##\n",
    "\n",
    "The first step is to import the dataset, shrec 16, a benchmark dataset for 3D mesh classification. We then lift each graph into our domain of choice, a combinatorial complex.\n",
    "\n",
    "We will also retrieve:\n",
    "- input signal on the nodes, edges, and faces for each of these combinatorial complexes, as that will be what we feed the model in input\n",
    "- the label associated to the combinatorial complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neighborhood structures and lift into combinatorial complex domain. ##\n",
    "\n",
    "\n",
    "Now, we lift each simplicial complex into a combinatorial complex. \n",
    "\n",
    "Now we retrieve the neighborhood structures (i.e. their representative matrices) that we will use to send messges on each combinatorial complex. In the case of this architecture, we need the boundary matrices (or incidence matrices) $B_1$, $B_2$ with shape $n_\\text{nodes} \\times n_\\text{1cells}$ and $n_\\text{1cells} \\times n_\\text{2cells}$ respectively. We also need the upwards adacency matrices for nodes and one-cells $A_{\\uparrow, 0}$, $A_{\\uparrow, 1}$, and the downwards adacency matrix for two cells, $A_{\\downarrow, 2}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_adjacency_down(incidence_mat):\n",
    "    \"\"\"Used to calculate downwards adjacency matrix from corresponding incidence matrix.\"\"\"\n",
    "    down_lap_2 = torch.sparse.mm(\n",
    "        incidence_mat.float().transpose(1, 0), incidence_mat.float()\n",
    "    )\n",
    "    down_lap_2 = down_lap_2\n",
    "    diag_tens = torch.zeros((incidence_mat.shape[1],))\n",
    "    one_indices, two_indices = incidence_mat.indices()\n",
    "    for i in two_indices:\n",
    "        diag_tens[i].add_(1)\n",
    "    D_down = torch.diag(diag_tens).to_sparse()\n",
    "    adj_matrix = D_down - down_lap_2\n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_list = []\n",
    "incidence_1_list = []\n",
    "incidence_2_list = []\n",
    "up_adjacency_0_list = []\n",
    "up_adjacency_1_list = []\n",
    "down_adjacency_2_list = []\n",
    "for simplex in simplexes:\n",
    "    cc = simplex.to_combinatorial_complex()\n",
    "    cc_list.append(cc)\n",
    "\n",
    "    incidence_1 = cc.incidence_matrix(rank=0, to_rank=1)\n",
    "    incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "    incidence_1_list.append(incidence_1)\n",
    "\n",
    "    incidence_2 = cc.incidence_matrix(rank=1, to_rank=2)\n",
    "    incidence_2 = torch.from_numpy(incidence_2.todense()).to_sparse()\n",
    "    incidence_2_list.append(incidence_2)\n",
    "\n",
    "    up_adjacency_0 = cc.adjacency_matrix(rank=0, via_rank=1)\n",
    "    up_adjacency_0 = torch.from_numpy(up_adjacency_0.todense()).to_sparse()\n",
    "    up_adjacency_0_list.append(up_adjacency_0)\n",
    "\n",
    "    up_adjacency_1 = cc.adjacency_matrix(rank=1, via_rank=2)\n",
    "    up_adjacency_1 = torch.from_numpy(up_adjacency_1.todense()).to_sparse()\n",
    "    up_adjacency_1_list.append(up_adjacency_1)\n",
    "\n",
    "    down_adjacency_2 = calc_adjacency_down(incidence_2)\n",
    "    down_adjacency_2_list.append(down_adjacency_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = max(ys) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Neural Network\n",
    "\n",
    "Using the HOANMCLayer class, we create a neural network with one topological layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 7]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = [x_0s[0].shape[-1], x_1s[0].shape[-1], x_2s[0].shape[-1]]\n",
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOANMCNN(torch.nn.Module):\n",
    "    \"\"\"Neural network implementation of Template for hypergraph classification.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    channels : list[int] length 3\n",
    "        Dimension of features at nodes, one-cells, and two-cells respectively\n",
    "    n_classes : int\n",
    "        Number of classes in output.\n",
    "    n_layers : 1\n",
    "        Amount of HOAN message passing layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, n_classes, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                HOANMCLayer(\n",
    "                    channels=channels,\n",
    "                )\n",
    "            )\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.linear_0 = torch.nn.Linear(channels[0], n_classes)\n",
    "        self.linear_1 = torch.nn.Linear(channels[1], n_classes)\n",
    "        self.linear_2 = torch.nn.Linear(channels[2], n_classes)\n",
    "\n",
    "        self.inter_aggr = Aggregation(\n",
    "            aggr_func=\"mean\",\n",
    "            update_func=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "        self.intra_aggr = Aggregation(\n",
    "            aggr_func=\"sum\",\n",
    "            update_func=None,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_0,\n",
    "        x_1,\n",
    "        x_2,\n",
    "        up_adjacency_0,\n",
    "        incidence_1,\n",
    "        up_adjacency_1,\n",
    "        incidence_2,\n",
    "        down_adjacency_2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward computation through layers. Then local aggregation, then tanh activation, then linear layer, then global aggregation\n",
    "        (with sigmoid update function).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_0 : tensor shape=[n_nodes, channels[0]]\n",
    "            Features on nodes.\n",
    "        x_1 : tensor shape=[n_1cells, channels[1]]\n",
    "            Features on one-cells.\n",
    "        x_2 : tensor shape=[n_2cells, channels[2]]\n",
    "            Features on two-cells.\n",
    "        up_adjacency_0 : tensor shape=[n_nodes, n_nodes]\n",
    "            Adjacency matrix for nodes across one-cells.\n",
    "        incidence_1 : tensor shape=[n_nodes, n_1cells]\n",
    "            Incidence matrix mapping one-cells to nodes.\n",
    "        up_adjacency_1 : tensor shape=[n_1cells, n_1cells]\n",
    "            Adjacency matrix for one-cells across two-cells.\n",
    "        incidence_2 : tensor shape=[n_1cells, n_2cells]\n",
    "            Incidence matrix mapping two-cells to one-cells.\n",
    "        down_adjacency_2 : tensor shape=[n_2cells, n_2cells]\n",
    "            Adjacency matrix for two-cells through one-cells.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x_0, x_1, x_2 = layer(\n",
    "                x_0,\n",
    "                x_1,\n",
    "                x_2,\n",
    "                up_adjacency_0,\n",
    "                incidence_1,\n",
    "                up_adjacency_1,\n",
    "                incidence_2,\n",
    "                down_adjacency_2,\n",
    "            )\n",
    "        x_0 = self.intra_aggr(torch.split(x_0, 1, dim=0))\n",
    "        x_1 = self.intra_aggr(torch.split(x_1, 1, dim=0))\n",
    "        x_2 = self.intra_aggr(torch.split(x_2, 1, dim=0))\n",
    "\n",
    "        x_0 = self.tanh(x_0)\n",
    "        x_1 = self.tanh(x_1)\n",
    "        x_2 = self.tanh(x_2)\n",
    "\n",
    "        x_0 = self.linear_0(x_0)\n",
    "        x_1 = self.linear_1(x_1)\n",
    "        x_2 = self.linear_2(x_2)\n",
    "\n",
    "        return self.inter_aggr([x_0, x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "x_0_train, x_0_test = train_test_split(x_0s, test_size=test_size, random_state=43)\n",
    "x_1_train, x_1_test = train_test_split(x_1s, test_size=test_size, random_state=43)\n",
    "x_2_train, x_2_test = train_test_split(x_2s, test_size=test_size, random_state=43)\n",
    "\n",
    "up_adj_0_train, up_adj_0_test = train_test_split(\n",
    "    up_adjacency_0_list, test_size=test_size, random_state=43\n",
    ")\n",
    "up_adj_1_train, up_adj_1_test = train_test_split(\n",
    "    up_adjacency_1_list, test_size=test_size, random_state=43\n",
    ")\n",
    "down_adj_2_train, down_adj_2_test = train_test_split(\n",
    "    down_adjacency_2_list, test_size=test_size, random_state=43\n",
    ")\n",
    "incidence_1_train, incidence_1_test = train_test_split(\n",
    "    incidence_1_list, test_size=test_size, random_state=43\n",
    ")\n",
    "incidence_2_train, incidence_2_test = train_test_split(\n",
    "    incidence_2_list, test_size=test_size, random_state=43\n",
    ")\n",
    "\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, random_state=43)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HOANMCNN(\n",
    "    channels=channels,\n",
    "    n_classes=num_classes,\n",
    ")\n",
    "model = model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 3.4497\n",
      "Test Loss: 3.4534\n",
      "Test Accuracy: 0.2000\n",
      "Epoch: 2 loss: 3.4228\n",
      "Test Loss: 3.4924\n",
      "Test Accuracy: 0.2000\n",
      "Epoch: 3 loss: 3.4005\n",
      "Test Loss: 3.4550\n",
      "Test Accuracy: 0.2500\n",
      "Epoch: 4 loss: 3.3898\n",
      "Test Loss: 3.4582\n",
      "Test Accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x_0, x_1, x_2, up_adj0, inc_1, up_adj1, inc_2, down_adj2, y in zip(\n",
    "        x_0_train,\n",
    "        x_1_train,\n",
    "        x_2_train,\n",
    "        up_adj_0_train,\n",
    "        incidence_1_train,\n",
    "        up_adj_1_train,\n",
    "        incidence_2_train,\n",
    "        down_adj_2_train,\n",
    "        y_train,\n",
    "    ):\n",
    "        x_0, x_1, x_2, y = (\n",
    "            torch.tensor(x_0).float().to(device),\n",
    "            torch.tensor(x_1).float().to(device),\n",
    "            torch.tensor(x_2).float().to(device),\n",
    "            torch.tensor(y).long().to(device),\n",
    "        )\n",
    "\n",
    "        up_adj0 = up_adj0.float().to(device)\n",
    "        inc_1 = inc_1.float().to(device)\n",
    "        up_adj1 = up_adj1.float().to(device)\n",
    "        inc_2 = inc_2.float().to(device)\n",
    "        down_adj2 = down_adj2.float().to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_0, x_1, x_2, up_adj0, inc_1, up_adj1, inc_2, down_adj2)\n",
    "        loss = loss_fn(y_hat.flatten(), y)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    if epoch_i % test_interval == 0:\n",
    "        corr = 0\n",
    "        with torch.no_grad():\n",
    "            for x_0, x_1, x_2, up_adj0, inc_1, up_adj1, inc_2, down_adj2, y in zip(\n",
    "                x_0_train,\n",
    "                x_1_train,\n",
    "                x_2_train,\n",
    "                up_adj_0_train,\n",
    "                incidence_1_train,\n",
    "                up_adj_1_train,\n",
    "                incidence_2_train,\n",
    "                down_adj_2_train,\n",
    "                y_train,\n",
    "            ):\n",
    "                x_0, x_1, x_2, y = (\n",
    "                    torch.tensor(x_0).float().to(device),\n",
    "                    torch.tensor(x_1).float().to(device),\n",
    "                    torch.tensor(x_2).float().to(device),\n",
    "                    torch.tensor(y).long().to(device),\n",
    "                )\n",
    "                x_list = [x_0, x_1, x_2]\n",
    "\n",
    "                up_adj0 = up_adj0.float().to(device)\n",
    "                inc_1 = inc_1.float().to(device)\n",
    "                up_adj1 = up_adj1.float().to(device)\n",
    "                inc_2 = inc_2.float().to(device)\n",
    "                down_adj2 = down_adj2.float().to(device)\n",
    "\n",
    "                y_hat = model(x_0, x_1, x_2, up_adj0, inc_1, up_adj1, inc_2, down_adj2)\n",
    "                test_loss = loss_fn(y_hat.flatten(), y)\n",
    "                if torch.argmax(y_hat) == y.item():\n",
    "                    corr += 1\n",
    "            acc = corr / (y_test.shape[0])\n",
    "            print(f\"Test Loss: {test_loss:.4f}\", flush=True)\n",
    "            print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
